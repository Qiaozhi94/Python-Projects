#### 数据提取

---

 **Beautiful Soup类的基本元素**

tag: **标签，最基本的信息组织单元，分别用<>和</>标明开头和结尾

**Name: **标签的名字，<p>...</p>的名字是‘p’，格式：<tag>.name

**Attributes: **标签的属性，字典形式组织，格式：<tag>.attrs

**NavigableString: **标签内非属性字符串，<>...</>中字符串，格式：<tag>.string

**Comment: **标签内字符串的注释部分，一种特殊的Comment类型

---

#### **标签树的遍历类型：**

1. 下行遍历

.contents: 子节点的列表，将<tag>所有儿子节点存入列表

.children: 子节点的迭代类型，与.contents类似，用于循环遍历儿子节点

~~~python
for child in soup.body.children：
    print(child)  #遍历儿子节点
~~~

~~~python
for child in soup.body.children：
    print(child)  #遍历子孙节点 #似乎有问题？
~~~

.descendants: 子孙节点的迭代类型，包含所有子孙节点，用于循环遍历

1. 上行遍历

.parent 节点的父亲标签

.parents 节点先辈标签的迭代类型，用于循环遍历先辈节点

~~~python
soup = BeautifulSoup(demo,"html.parser")
for parent in soup.a.parents:
    if parent is None:
             print(parent)
    else:
             print(parent.name)
             
p
body
html
[documents]
~~~



1. 平行遍历

.next_sibling 返回按照HTML文本顺序的下一个平行节点标签

.previous_sibling 返回按照HTML文本顺序的上一个平行节点标签

.next_siblings 迭代类型，返回按照HTML文本顺序的后续所有平行节点标签

.previous_siblings 迭代类型，返回按照HTML文本顺序的前续所有平行节点标签

~~~python
for sibling in soup.a.next_siblings:
    print(sibling)    #遍历后续的节点
~~~

~~~python
for sibling in soup.a.previous_siblings:
    print (sibling)   #遍历前续的节点
~~~

---

信息标记：

- 标记后的信息可形成信息组织结构，增加了信息维度
- 标记后的信息可用于通信、存储或展示
- 标记的结构与信息一样具有重要价值
- 标记后的信息更有利于程序理解和运用

三种信息标记形式：XML、JSON、YAML

XML: eXtensible Markup Language 扩展标记语言

~~~xml
<img src="china.jpg" size="10">...</img>  
#与HTML相关格式非常相似
#xml是基于html发展出来的一种通用表达形式
~~~

JSON: JavaScript Object Notation 是JavaScript语言中面向对象的一种表达形式（格式）。有类型(有双引号）的键值对 key: value，类似于Python中的字典

YAML: YAML Ain't Markup Language 全称是一种递归的定义。无类型（无双引号）的键值对key: value，-用来表达并列关系。|用来表达整块数据

三者比较：

| 标记形式 | 特点                                                       | 比较                                 |
| -------- | ---------------------------------------------------------- | ------------------------------------ |
| XML      | 最早的通用信息标记语言，可扩展性好，但繁琐。               | Internet上的信息交互与传递           |
| JSON     | 有类型的信息方式来标记信息，适合程序处理（JS），较XML简洁  | 移动应用云端和节点的信息通信，无注释 |
| YAML     | 采用无类型的信息方式来标记信息，文本信息比例最高，可读性好 | 各类系统的配置文件中，有注释易读     |

---

信息提取的一般方法：

1. 完整解析信息的标记形式，在提取关键信息。

XML JSON YAML  需要标记解析器 ，例如：bs4库中的标记树遍历

优点：信息解析准确

缺点：提取过程繁琐，速度慢

2. 无视标记形式，直接搜索关键信息

搜索

对信息的文本查找函数即可。

优点：提取过程简洁，速度较快

缺点：提取结果准确性与信息内容直接相关

3. 融合方法：结合形式解析和搜索方法，提取关键信息

XML JSON YSML 搜索

需要标记解析器及文本查找函数

过程举例：

提取HTML中所有的URL链接

思路：1. 搜索到所有的<a>标签

           2. 解析<a>标签格式，提取href后的链接内容

---

#### **XPath语法与lxml模块：**

**XPath基本语法：**

Xpath，全称 `XML Path Language`，及XML路径语言，是一门在XML文档中查找信息的语言，最初是用来搜寻XML文档的，但是它同样适用于HTML文档的搜索。**使用xpath的时候可以用到chrome的xpath helper用以快速验证结果。**

**常用规则：**

|        表达式        |          描述           |
| :------------------: | :---------------------: |
|       a/text()       |      获取a下的文本      |
|      a//text()       |  获取a下所有元素的文本  |
| //a[text()='下一页'] | 获取文本为下一页的a元素 |

**获取属性：**

|  表达式  |           描述           |
| :------: | :----------------------: |
| nodename |  选取此节点的所有子节点  |
|    /     | 从当前节点选取直接子节点 |
|    //    |  从当前节点选取子孙节点  |
|    .     |       选取当前节点       |
|    ..    |   选取当前节点的父节点   |
|    @     |         选取属性         |
|    *     |     匹配任何元素节点     |
|    @*    |     匹配任何属性节点     |
|  node()  |    匹配任何类型的节点    |

**常用表达式：**

|               路径表达式               |                             结果                             |
| :------------------------------------: | :----------------------------------------------------------: |
| /bookstore/book[1] （注意下标从1开始） |          选取属于 bookstore 子元素的第一个book元素           |
|        /bookstore/book[last()]         |        选取属于 bookstore 子元素的最后一个 book 元素         |
|       /bookstore/book[last()-1]        |       选取属于 bookstore 子元素的倒数第二个 book 元素        |
|     /bookstore/book[position()<3]      |       选取属于 bookstore 子元素的 最前面两个 book 元素       |
|             //title[@lang]             |           选取所有拥有名为 lang 的属性的title元素            |
|            //title/a/@lang             |              选取title下的a标签内lang属性的元素              |
|          //title[@lang='eng']          |            选取所有 lang 属性为 eng 的 title 元素            |
|      /bookstore/book[price>35.00]      | 选取 bookstore 元素下所有 book 元素，它们的 price 元素值大于 35.00 |
|   /bookstore/book[price>35.00]/title   | 选取 bookstore 元素中的 book 元素的所有 title 元素，且 price 元素的值大于35.00 |
|              /bookstore/*              |                 选取 bookstore 的所有子元素                  |
|                  //*                   |                     选取文档中的所有元素                     |
|           //node()/meta[]/@*           |         选取 html 下面任意节点的 meta 节点的所有属性         |
|              //title[@*]               |                选取所有带有属性的 title 元素                 |
|      //book/title\|// book/price       |           选取 book 元素的所有 title 和 price 元素           |
|            //title\|//price            |             选取文档中的所有 title 和 price 元素             |
|    //bookstore/book/title\|//price     | 选取属于 bookstore 元素的 book 元素的所有 title 元素，以及文档中所有的 price 元素 |

**lxml的使用：**解析html代码

1. 解析html字符串：使用"lxml.etree.HTML"进行解析。示例代码如下：

~~~python
htmlElement = etree.HTML(text)
print(etree.tostring(htmlElement,encoding='utf-8'.decode='utf-8'))
~~~

2. 解析html文件：使用"lxml.etree.parse"进行解析，示例代码如下：

~~~python
htmlElement = etree.parse("example.html")
print(etree.tostring(htmlElement,encoding='utf-8'.decode='utf-8'))
~~~

​         注意点：这个函数默认使用的是"xml"解析器，所以如果碰到一些不规范的"html"代码的时候就会解析锁雾，这时候就需要自己创建一个"html"解析器：

~~~python
parser = etree.HTMLParser(encoding='utf-8')
htmlElement = etree.parse("example.html", parser=parser)
print(etree.tostring(htmlElement,encoding='utf-8'.decode='utf-8'))
~~~

~~~python
from lxml import etree

text = ''' <div> <ul> 
            <li class="item-1"><a href="link1.html">first item</a></li> 
            <li class="item-1"><a href="link2.html">second item</a></li> 
            <li class="item-inactive"><a href="link3.html">third item</a></li> 
            <li class="item-1"><a href="link4.html">fourth item</a></li> 
            <li class="item-0"><a href="link5.html">fifth item</a> 
            </ul> </div> '''

html = etree.HTML(text)

print(html) # <Element html at 0x1f1007c9d08>
print(etree.tostring(html).decode('utf-8'))

# 获取 class 为 item-1 li 下的 a 的 href
ret1 = html.xpath('//li[@class="item-1"]/a/@href')
print(ret1)

# 获取 class 为 item-1 li 下的文本
ret2 = html.xpath("//li[@class='item-1']/a/text()")
print(ret2)

# 把 url 和 文本组成字典
# 如果其中一个获取失败或者没有数据，则url 和 title 就不是原来对应的结果
for i in ret1:
    item = {}
    item['url'] = i
    item['title'] = ret2[ret1.index(i)]
    print(item)

# 改进
ret3 = html.xpath('//li[@class="item-1"]')
for i in ret3:
    item = {}
    item['url'] = i.xpath('./a/@href')[0] if len(i.xpath('./a/@href')) else None  # ./a/@href 表示当前节点下的
    item['title'] = i.xpath('./a/text()')[0] if len(i.xpath('./a/text()')) else None
    print(item)

~~~

lxml结合xpath注意事项：

1. 使用'xpath'语法， 应该使用'Element.xpath'方法来执行xpath的选择，如下：

~~~python
trs = html.xpath("//re[position()>1]")   #xpath函数的返回的值永远是一个列表
~~~

2. 获取某个标签的属性：

~~~python
href = html.xpath("//a/@href")   #获取a标签的href属性对应的值
~~~

3. 获取文本，是通过xpath下的text()函数：

~~~python
text = html.xpath("./td[4]/text()")[0]   
~~~

4. 在某个标签下如要在执行xpath函数，获取这个标签下的子孙元素，就应该在//前加一个点，代表在当前元素下获取：

~~~python
text = tr.xpath("./td[4]/text()")[0]
~~~



