## 机器学习基础

1. **机器学习的类型：**
   * 是否在人类监督下进行训练（监督、半监督、非监督与强化学习）
   * 是否可以动态渐进学习（在线学习&批量学习）
   * 是否只是通过简单的比较新的数据点和已知数据点，还是在训练数据中进行模式识别，以建立一个预测模型，就像科学家所做的那样（基于实例学习&基于模型学习）

2. **机器学习与深度学习的区别：**
机器学习与深度学习的区别可以参照如下两图解，均摘取自《Deep Learning》（即《深度学习》花书版，作者为 Ian Goodfellow, Yoshua Bengio, Aaron Courville）

![image](https://qiaozhi94.oss-cn-beijing.aliyuncs.com/Github/Python-Projects/2022-01-31-22-27-01-image.png)
![image](https://qiaozhi94.oss-cn-beijing.aliyuncs.com/Github/Python-Projects/2022-01-31-22-27-57-image.png)
该图解非常清晰的展示了人工智能（AI），机器学习（Machine Learning），表示学习（Representation Learning）和深度学习（Deep Learning）之间的从属关系，下面的引用部分深入介绍个各部分的含义与典型例子。

> 一些人工智能项目力求将关于世界的知识用形式化的语言进行硬编码（hard-code）。计算机可以使用逻辑推理规则来自动理解这些形式化语言的声明（statements）。这就是众所周知的人工智能的**知识库（Knowledge bases）方法**。然而，这些项目最终都没有取得重大的成功。其中最著名的项目是Cyc。Cyc包括一个推断引擎和一个使用CycL语言描述的声明数据库。这些声明是由人类监督者输入的。这是一个笨拙的过程。人类设法设计出足够复杂的形式化规则来精确地描述世界。
> 依靠硬编码的知识体系面对的困难声明，AI系统需要具备自己获取知识的能力，即从原始数据中提取模式的能力。这种能力被称为**机器学习**（Machine Learning）。引入机器学习使计算机能够解决设计现实世界知识的问题，并能做出看似主观的决策。比如，一个被称为**逻辑回归**(Logistic Regression)的简单机器学习算法可以决定是否建议剖腹产。而同样是简单机器学习算法的**朴素贝叶斯**（Navie Bayes）则可以区分垃圾邮件和正常电子邮件。
> 这些简单的机器学习算法的性能很大程度上依赖于给定数据的**表示**（Representation）。在整个计算机科学乃至日常生活中，对表示的依赖都是一个普遍现象。许多人工智能任务都可以通过以下方式解决：先提取一个合适的特征集，然后将这些特征提供给简单的机器学习算法。然而对于许多任务来说，我们很难指导应该提取哪些特征，比如假设我们像编写一个程序来检测照片中的汽车。我们知道汽车有轮子，所以我们可能会想到车轮的存在是否可以作为特征。可惜的是，受到场景差异的影响，如光线和阴影等等，我们难以准确的根据像素值来描述车轮看上去像什么。
> 
> 解决这个问题的途径之一是使用机器学习来发掘表示本身，而不仅仅把表示映射到输出。这种方法我们称之为**表示学习**（Representation Learning）。学习到的表示往往比手动设计的表示表现得更好。并且他们只需要最少的人工干预，就能让AI系统迅速适应新的任务。
> 
> 表示学习算法的典型例子是**自编码器**（autoencoder）。自编码器由一个编码器函数和一个解码器函数组合而成。编码器函数将输入数据转换为一种不同的表示，儿解码器函数则将这个新的表示转换为原来的形式。我们期望当输入数据经过编码器和解码器之后尽可能多得保留信息，同时希望新的表示有各种好的特性，这也是自编码器的训练目标。为了实现不同的特性，我们可以设计不同形式的自编码器。
> ......
> 
> 当设计特征或设计用于学习特征的算法时，我们的目标通常是分离出能解释观察数据的**变差因素**（Factors of Variation）。例如当分析语音记录时，变差因素包括说话者的年龄，性别，他们的口音与他们正在说的词语。当分析汽车的图像时，变差因素包括汽车的位置，他们的颜色和太阳的角度亮度等。
> 
> 在许多现实的人工智能应用中，困难主要源于多个变差因素同时影响着我们能够观察到的每一个数据。比如在一张包含红色汽车的图片中，其单个像素在夜间可能会非常接近黑色。汽车轮廓的形状取决于视角。大多数应用需要我们理清变差因素并忽略我们不关心的因素。
> 
> 显然，从原始数据中提取如此高层次、抽象的特征是十分困难的。许多诸如说话口音这样的变差因素，只能通过对数据进行复杂的、接近人类水平的理解来辨识。这几乎与获得原问题的表示一样困难。因此，表示学习似乎并不能帮助我们。
> 
> **深度学习**（Deep Learning）通过其他较为简单的表示来表达复杂表示，解决了表示学习中的核心问题。深度学习让计算机通过简单概念构建复杂的概念，深度学习的典型例子是前馈深度网络或**多层感知机**（Multilayer Perception，MLP）。多层感知机仅仅是一个将一组输入值映射到输出值的数学函数。该函数由许多较简单的函数复合而成。我们可以认为不同数学函数的每一次应用都为输入提供了新的表示。

3. **监督学习**
**监督学习**中，用来训练算法的训练数据中包含了答案，称为标签。一个最典型的监督学习的任务是**分类**（例如给垃圾邮件分类，即垃圾邮件过滤器）。另一类典型任务为预测目标数值，即**回归**。例如给定一些特征（如里程数，车龄，品牌等）称为预测值，来预测一辆车汽车的价格。
**一些比较重要的监督学习算法：**
* K-近邻算法
* 线性回归
* 逻辑回归
* 支持向量机（SVM）
* 决策树与随机森林
* 神经网络

4. **非监督学习**
**非监督学习**中训练数据是没有添加标签的。可以使用**聚类算法**检测相似博客访客的分组，如果使用**层次聚类算法**，则有可能会细分每个分组为更小的组。这可以帮助你为每个分组定位博文。另一类重要的非监督任务是**异常检测**，例如可以检测异常的信用卡转账以防欺诈，检测制造缺陷，或在训练前自动从训练数据集中去除异常值。最后，另一个常见的非监督任务是**关联规则学习**，它的目标是挖掘大量数据已发现属性间有趣的关系，例如发现去超市的客户中可能发现买烧烤酱和薯片的人一般也会买牛排，因此可以将以上商品放置在一起方便客户。
**一些最重要的非监督学习的算法：**
* 聚类
	* K均值
	* 层次聚类分析（HCA）
	* 期望最大值
* 可视化和降维
	* 主成分分析（PCA）
	* 核主成分分析
	* 局部线性嵌入（LLE）
	* t-分布邻域嵌入算法（t-SNE）
* 关联性规则学习
	* Apriori算法
	* Eclat算法

5. **半监督学习**
一些算法可以处理部分带标签的训练数据，通常是大量不带标签数据加上小部分带标签数据，称为半监督学习。
半监督学习的典型例子之一就是一些照片存储服务，如Google Photos，它会将上传的照片中的人像进行聚类算法分析后归纳出一共出现了几个人像等等。多数半监督学习算法是非监督与监督算法的结合。例如深度信念网络是基于被称为互相叠加的受限玻尔兹曼机的非监督组件。RBM是先用非监督方法进行训练，再用监督学习方法对整个系统进行微调。

6. **强化学习**
强化学习与之前的学习类型非常不同。学习系统在这里被称为**智能体**（agent），可以对环境进行观察，选择和执行动作，并获得奖励作为回报（负奖励为惩罚）。智能体必须自己学习哪个是最佳方法（称为策略，policy），以此获得长久的最大奖励。策略决定了智能体在给定情况下应该采取的行动。DeepMind的AlphaGo就是强化学习的典型例子。它通过分析数百万盘棋局学习制胜策略，然后自己与自己下棋。
![image](https://qiaozhi94.oss-cn-beijing.aliyuncs.com/Github/Python-Projects/3.jpg)

7. **根据能从导入的数据流进行持续学习作为判断分为：批量学习与在线学习**
**批量学习**
在批量学习中，系统不能进行持续学习：必须使用所有的可用数据进行训练。这通常会占用大量的时间和计算资源，所以通常是线下进行的。一般是每24小时或每周训练一个新系统。
**在线学习**
在在线学习中，使用数据实例持续进行训练，可以一次一个或一次几个实例。每个学习步骤都很快且廉价，所以系统可以动态的获取最新的数据。
在线学习很适合系统接收连续流的数据（例如股票价格），且需要自动对改变做出调整。如果计算资源有限，则在线学习是一个不错的解决方案：一旦在线学习系统学习了新的数据实例，它就不再需要这些数据，就可以扔掉这些数据（除非出于某种原因想回滚到之前的状态），这样可以节省大量的空间。
在线学习算法也适用于超大数据集（一台计算机不足以存储它）上训练系统（称为核外学习）。算法每次只加载部分数据，用这些数据进行训练，然后重复这个过程，直到使用完所有数据。
在线学习的一个重要参数是，它可以多快的适应数据的改变：这称为学习速率。如果你设定一个高学习速率，系统就可以快速适应新数据，但是也会快速忘记老数据。相反，如果你设定多个学习速率较低，系统的惰性就会增强，即：它学的更慢，但对新数据中的噪声或没有代表性的数据点结果不那么敏感。
在线学习的挑战之一是，如果坏数据被用来进行训练，系统的性能就会逐渐下降。要减小这种风险，就需要监督人密集监测，如果检测到性能下降，就要快速关闭（或者回滚到之前的状态）。监督人可能还要监测输入数据们，对反应异常的数据作出反应（比如使用异常检测算法）。

8. **根据其如何进行归纳推广作为判断可以分为：基于的实例学习与基于模型学习**

![image](https://qiaozhi94.oss-cn-beijing.aliyuncs.com/Github/Python-Projects/1.jpg)

大多数机器学习人物都是关于预测的。这意味着给定一定数量的训练样本，系统就需要推广到之后没见过的样本。对训练数据集有很好的性能还不够，真正的目标是对新实例预测的性能。
也许最简单的学习形式是记忆学习，如果使用该方法做个垃圾邮件检测器，只是标记和已知的垃圾邮件相同的邮件是不够的，也要能标记类似垃圾邮件的邮件。此时可以测量两封邮件的相似性，一个简单的相似度测量方法是统计两封邮件包含相同单词的数量（如果一封邮件中含有许多垃圾邮件中的词，则被标记为垃圾邮件）。**这种方法称为基于实例学习：系统先用记忆学习学习实例，然后使用相似度测量推广到新的例子。**

![image](https://qiaozhi94.oss-cn-beijing.aliyuncs.com/Github/Python-Projects/2.jpg)

**另一个从样本集归纳的方法就是建立这些样本的模型，然后使用这个模型进行预测。这种方法被称为基于模型学习。**
基于模型学习的路径如下：
* 研究数据
* 选择模型
* 用训练数据进行训练
* 使用模型对新案例进行预测

---
## 机器学习的主要挑战
导致机器学习失败的因素主要为错误的算法和错误的数据

1. **错误的数据：**
* 训练数据量不足：需要大量的数据
* 没有代表性的训练数据：样本偏差
* 低质量数据：需要进行数据清洗以提升数据质量
* 不相关的特征：机器学习成功的关键之一是用好的特征进行训练，这个过程称为特征工程，包括：特征选择，特征提取和收集新数据创建新特征

2. **错误的算法：
* 过拟合训练数据：模型在训练数据表现上很好，但推广效果不好
	过拟合通常发生在相对于训练数据的量和噪声，模型过于复杂的情况。可能的解决方法为：简化模型，手机更多的训练数据和减小训练数据的噪声。
	限定一个模型以让它更简单并且降低过拟合的风险被称为正则化，正则化的度可以用一个超参数控制。超参数是一个学习算法的参数（而不是模型的），调节超参数是创建机器学习算法中非常重要的一部分
* 欠拟合训练数据：
	解决方法为：选择一个更强大的模型，带有更多的参数
					       用更好的特征训练学习算法（特征工程）
							减小对模型的限制（比如减小正则化超参数）

---
## 总结回顾

1. 机器学习是让机器通过学习数据对某些任务做的更好，而不使用确定的代码规则
2. 有许多不同类型的机器学习系统：监督与非监督，批量与在线，基于实例与基于模型等等
3. 在机器学习项目中，我们从训练集中收集数据，然后对学习算法进行训练。如果算法是基于模型的，就调节一些参数，让模型拟合到训练集（即对训练集本身做出好的预测），然后希望它对新样本也能有好的预测。如果算法是基于实例的，就是用记忆学习样本，然后用相似度推广到新实例
4. 如果训练集太小，数据没有代表性，含有噪声或掺杂不相关的特征（垃圾进，垃圾出），系统的性能就不会好。最后模型不能太简单（会发生欠拟合）或太复杂（会发生过拟合）

---
## 测试与确认

可以将数据集分为两个集合：训练集与测试集（一般80%的数据训练，20%的数据测试）。训练集进行训练，用测试集进行测试。对新样本的错误称为推广错误，通过模型对测试集的评估，可以预估这个错误。
如果训练错误率低，但推广错误率高，意味着模型对训练数据过拟合。
